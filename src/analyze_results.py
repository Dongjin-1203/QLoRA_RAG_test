"""
ì‹¤í—˜ ê²°ê³¼ ë¶„ì„ ë° ì‹œê°í™”

JSON ê²°ê³¼ íŒŒì¼ì„ ì½ì–´ì„œ ë‹¤ì–‘í•œ ê·¸ë˜í”„ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.
"""

import json
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from pathlib import Path
from typing import Dict, List, Any
import numpy as np

# í•œê¸€ í°íŠ¸ ì„¤ì • (matplotlib)
plt.rcParams['font.family'] = 'DejaVu Sans'
plt.rcParams['axes.unicode_minus'] = False


class ResultAnalyzer:
    """ì‹¤í—˜ ê²°ê³¼ ë¶„ì„ í´ë˜ìŠ¤"""
    
    def __init__(self, result_file_path: str):
        """
        ì´ˆê¸°í™”
        
        Args:
            result_file_path: JSON ê²°ê³¼ íŒŒì¼ ê²½ë¡œ
        """
        self.result_file = Path(result_file_path)
        self.results = self._load_results()
        self.df = self._results_to_dataframe()
        
        # ë¶„ì„ ê²°ê³¼ ì €ì¥ ë””ë ‰í† ë¦¬
        self.analysis_dir = self.result_file.parent / "analysis"
        self.analysis_dir.mkdir(parents=True, exist_ok=True)
        
        print(f"âœ… ResultAnalyzer ì´ˆê¸°í™” ì™„ë£Œ")
        print(f"   ê²°ê³¼ íŒŒì¼: {self.result_file}")
        print(f"   ë¶„ì„ ì €ì¥ ê²½ë¡œ: {self.analysis_dir}")
    
    def _load_results(self) -> Dict[str, Any]:
        """JSON ê²°ê³¼ íŒŒì¼ ë¡œë“œ"""
        with open(self.result_file, 'r', encoding='utf-8') as f:
            return json.load(f)
    
    def _results_to_dataframe(self) -> pd.DataFrame:
        """ê²°ê³¼ë¥¼ DataFrameìœ¼ë¡œ ë³€í™˜"""
        all_rows = []
        
        for dist_type, dist_results in self.results['results'].items():
            for result in dist_results:
                row = {
                    'distribution': dist_type,
                    'model': result['model'],
                    'query': result['query'],
                    'category': result.get('category', 'unknown'),
                    'success': result['success'],
                    'elapsed_time': result['elapsed_time'],
                    'used_retrieval': result.get('used_retrieval', False),
                    'query_type': result.get('query_type', 'unknown'),
                    'search_mode': result.get('search_mode', 'none'),
                    'total_tokens': result.get('usage', {}).get('total_tokens', 0),
                    'prompt_tokens': result.get('usage', {}).get('prompt_tokens', 0),
                    'completion_tokens': result.get('usage', {}).get('completion_tokens', 0),
                }
                all_rows.append(row)
        
        return pd.DataFrame(all_rows)
    
    def plot_time_comparison(self, figsize=(12, 6)):
        """ì‘ë‹µ ì‹œê°„ ë¹„êµ ê·¸ë˜í”„"""
        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=figsize)
        
        # ì„±ê³µí•œ ê²°ê³¼ë§Œ ì‚¬ìš©
        df_success = self.df[self.df['success'] == True].copy()
        
        # 1. ëª¨ë¸ë³„ í‰ê·  ì‘ë‹µ ì‹œê°„
        model_time = df_success.groupby('model')['elapsed_time'].agg(['mean', 'std'])
        
        ax1.bar(model_time.index, model_time['mean'], yerr=model_time['std'], 
                capsize=5, alpha=0.7, color=['#2ecc71', '#3498db', '#e74c3c'])
        ax1.set_title('Average Response Time by Model', fontsize=14, fontweight='bold')
        ax1.set_ylabel('Time (seconds)', fontsize=12)
        ax1.set_xlabel('Model', fontsize=12)
        ax1.grid(axis='y', alpha=0.3)
        
        # 2. Distributionë³„ ì‘ë‹µ ì‹œê°„
        pivot_data = df_success.pivot_table(
            values='elapsed_time', 
            index='model', 
            columns='distribution', 
            aggfunc='mean'
        )
        
        x = np.arange(len(pivot_data.index))
        width = 0.35
        
        if 'in_distribution' in pivot_data.columns:
            ax2.bar(x - width/2, pivot_data['in_distribution'], width, 
                   label='In-Distribution', alpha=0.8, color='#2ecc71')
        
        if 'out_distribution' in pivot_data.columns:
            ax2.bar(x + width/2, pivot_data['out_distribution'], width, 
                   label='Out-Distribution', alpha=0.8, color='#e74c3c')
        
        ax2.set_title('Response Time: In vs Out Distribution', fontsize=14, fontweight='bold')
        ax2.set_ylabel('Time (seconds)', fontsize=12)
        ax2.set_xlabel('Model', fontsize=12)
        ax2.set_xticks(x)
        ax2.set_xticklabels(pivot_data.index, rotation=15, ha='right')
        ax2.legend()
        ax2.grid(axis='y', alpha=0.3)
        
        plt.tight_layout()
        
        # ì €ì¥
        output_file = self.analysis_dir / "time_comparison.png"
        plt.savefig(output_file, dpi=300, bbox_inches='tight')
        plt.close()
        
        print(f"âœ… ì‘ë‹µ ì‹œê°„ ê·¸ë˜í”„ ì €ì¥: {output_file}")
    
    def plot_token_comparison(self, figsize=(12, 6)):
        """í† í° ì‚¬ìš©ëŸ‰ ë¹„êµ ê·¸ë˜í”„"""
        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=figsize)
        
        # ì„±ê³µí•œ ê²°ê³¼ë§Œ ì‚¬ìš©
        df_success = self.df[self.df['success'] == True].copy()
        
        # 1. ëª¨ë¸ë³„ í‰ê·  í† í° ì‚¬ìš©ëŸ‰
        model_tokens = df_success.groupby('model')['total_tokens'].agg(['mean', 'std'])
        
        ax1.bar(model_tokens.index, model_tokens['mean'], yerr=model_tokens['std'],
                capsize=5, alpha=0.7, color=['#2ecc71', '#3498db', '#e74c3c'])
        ax1.set_title('Average Token Usage by Model', fontsize=14, fontweight='bold')
        ax1.set_ylabel('Tokens', fontsize=12)
        ax1.set_xlabel('Model', fontsize=12)
        ax1.grid(axis='y', alpha=0.3)
        
        # 2. Distributionë³„ í† í° ì‚¬ìš©ëŸ‰
        pivot_data = df_success.pivot_table(
            values='total_tokens',
            index='model',
            columns='distribution',
            aggfunc='mean'
        )
        
        x = np.arange(len(pivot_data.index))
        width = 0.35
        
        if 'in_distribution' in pivot_data.columns:
            ax2.bar(x - width/2, pivot_data['in_distribution'], width,
                   label='In-Distribution', alpha=0.8, color='#2ecc71')
        
        if 'out_distribution' in pivot_data.columns:
            ax2.bar(x + width/2, pivot_data['out_distribution'], width,
                   label='Out-Distribution', alpha=0.8, color='#e74c3c')
        
        ax2.set_title('Token Usage: In vs Out Distribution', fontsize=14, fontweight='bold')
        ax2.set_ylabel('Tokens', fontsize=12)
        ax2.set_xlabel('Model', fontsize=12)
        ax2.set_xticks(x)
        ax2.set_xticklabels(pivot_data.index, rotation=15, ha='right')
        ax2.legend()
        ax2.grid(axis='y', alpha=0.3)
        
        plt.tight_layout()
        
        # ì €ì¥
        output_file = self.analysis_dir / "token_comparison.png"
        plt.savefig(output_file, dpi=300, bbox_inches='tight')
        plt.close()
        
        print(f"âœ… í† í° ì‚¬ìš©ëŸ‰ ê·¸ë˜í”„ ì €ì¥: {output_file}")
    
    def plot_rag_usage(self, figsize=(10, 6)):
        """RAG ì‚¬ìš© íŒ¨í„´ ë¶„ì„"""
        fig, ax = plt.subplots(figsize=figsize)
        
        # ëª¨ë¸ë³„ RAG ì‚¬ìš©ë¥  ê³„ì‚°
        rag_usage = self.df.groupby('model').agg({
            'used_retrieval': lambda x: (x.sum() / len(x) * 100)
        }).round(2)
        
        colors = ['#2ecc71', '#3498db', '#e74c3c']
        bars = ax.bar(rag_usage.index, rag_usage['used_retrieval'], 
                     alpha=0.7, color=colors)
        
        # ë§‰ëŒ€ ìœ„ì— í¼ì„¼íŠ¸ í‘œì‹œ
        for bar in bars:
            height = bar.get_height()
            ax.text(bar.get_x() + bar.get_width()/2., height,
                   f'{height:.1f}%',
                   ha='center', va='bottom', fontsize=11, fontweight='bold')
        
        ax.set_title('RAG Usage Rate by Model', fontsize=14, fontweight='bold')
        ax.set_ylabel('Usage Rate (%)', fontsize=12)
        ax.set_xlabel('Model', fontsize=12)
        ax.set_ylim(0, 105)
        ax.grid(axis='y', alpha=0.3)
        
        plt.tight_layout()
        
        # ì €ì¥
        output_file = self.analysis_dir / "rag_usage.png"
        plt.savefig(output_file, dpi=300, bbox_inches='tight')
        plt.close()
        
        print(f"âœ… RAG ì‚¬ìš© íŒ¨í„´ ê·¸ë˜í”„ ì €ì¥: {output_file}")
    
    def plot_overfitting_analysis(self, figsize=(12, 8)):
        """ê³¼ì í•© ë¶„ì„: In-Distribution vs Out-Distribution ì„±ëŠ¥ ì°¨ì´"""
        # ì„±ê³µí•œ ê²°ê³¼ë§Œ ì‚¬ìš©
        df_success = self.df[self.df['success'] == True].copy()
        
        fig, axes = plt.subplots(2, 2, figsize=figsize)
        
        # 1. ì‘ë‹µ ì‹œê°„ ì°¨ì´
        time_pivot = df_success.pivot_table(
            values='elapsed_time',
            index='model',
            columns='distribution',
            aggfunc='mean'
        )
        
        if 'in_distribution' in time_pivot.columns and 'out_distribution' in time_pivot.columns:
            time_diff = time_pivot['out_distribution'] - time_pivot['in_distribution']
            
            axes[0, 0].bar(time_diff.index, time_diff.values, 
                          color=['green' if x < 0 else 'red' for x in time_diff.values],
                          alpha=0.7)
            axes[0, 0].axhline(y=0, color='black', linestyle='-', linewidth=0.8)
            axes[0, 0].set_title('Response Time Gap (Out - In)', fontsize=12, fontweight='bold')
            axes[0, 0].set_ylabel('Time Difference (seconds)', fontsize=10)
            axes[0, 0].grid(axis='y', alpha=0.3)
        
        # 2. í† í° ì‚¬ìš©ëŸ‰ ì°¨ì´
        token_pivot = df_success.pivot_table(
            values='total_tokens',
            index='model',
            columns='distribution',
            aggfunc='mean'
        )
        
        if 'in_distribution' in token_pivot.columns and 'out_distribution' in token_pivot.columns:
            token_diff = token_pivot['out_distribution'] - token_pivot['in_distribution']
            
            axes[0, 1].bar(token_diff.index, token_diff.values,
                          color=['green' if x < 0 else 'red' for x in token_diff.values],
                          alpha=0.7)
            axes[0, 1].axhline(y=0, color='black', linestyle='-', linewidth=0.8)
            axes[0, 1].set_title('Token Usage Gap (Out - In)', fontsize=12, fontweight='bold')
            axes[0, 1].set_ylabel('Token Difference', fontsize=10)
            axes[0, 1].grid(axis='y', alpha=0.3)
        
        # 3. ì„±ê³µë¥  ë¹„êµ
        success_pivot = self.df.pivot_table(
            values='success',
            index='model',
            columns='distribution',
            aggfunc=lambda x: (x.sum() / len(x) * 100)
        )
        
        x = np.arange(len(success_pivot.index))
        width = 0.35
        
        if 'in_distribution' in success_pivot.columns:
            axes[1, 0].bar(x - width/2, success_pivot['in_distribution'], width,
                          label='In-Distribution', alpha=0.8, color='#2ecc71')
        
        if 'out_distribution' in success_pivot.columns:
            axes[1, 0].bar(x + width/2, success_pivot['out_distribution'], width,
                          label='Out-Distribution', alpha=0.8, color='#e74c3c')
        
        axes[1, 0].set_title('Success Rate Comparison', fontsize=12, fontweight='bold')
        axes[1, 0].set_ylabel('Success Rate (%)', fontsize=10)
        axes[1, 0].set_xticks(x)
        axes[1, 0].set_xticklabels(success_pivot.index, rotation=15, ha='right')
        axes[1, 0].legend()
        axes[1, 0].grid(axis='y', alpha=0.3)
        axes[1, 0].set_ylim(0, 105)
        
        # 4. ê³¼ì í•© ì§€ìˆ˜ (Performance Gap)
        if 'in_distribution' in time_pivot.columns and 'out_distribution' in time_pivot.columns:
            # ê°„ë‹¨í•œ ê³¼ì í•© ì§€ìˆ˜: (Out ì‹œê°„ - In ì‹œê°„) / In ì‹œê°„ * 100
            overfitting_index = ((time_pivot['out_distribution'] - time_pivot['in_distribution']) / 
                                time_pivot['in_distribution'] * 100)
            
            colors_custom = ['green' if x < 10 else 'orange' if x < 30 else 'red' 
                           for x in overfitting_index.values]
            
            axes[1, 1].bar(overfitting_index.index, overfitting_index.values,
                          color=colors_custom, alpha=0.7)
            axes[1, 1].axhline(y=0, color='black', linestyle='-', linewidth=0.8)
            axes[1, 1].axhline(y=10, color='orange', linestyle='--', linewidth=0.8, alpha=0.5)
            axes[1, 1].axhline(y=30, color='red', linestyle='--', linewidth=0.8, alpha=0.5)
            axes[1, 1].set_title('Overfitting Index (Time-based)', fontsize=12, fontweight='bold')
            axes[1, 1].set_ylabel('Performance Gap (%)', fontsize=10)
            axes[1, 1].grid(axis='y', alpha=0.3)
        
        plt.tight_layout()
        
        # ì €ì¥
        output_file = self.analysis_dir / "overfitting_analysis.png"
        plt.savefig(output_file, dpi=300, bbox_inches='tight')
        plt.close()
        
        print(f"âœ… ê³¼ì í•© ë¶„ì„ ê·¸ë˜í”„ ì €ì¥: {output_file}")
    
    def generate_summary_report(self):
        """ì¢…í•© ìš”ì•½ ë³´ê³ ì„œ ìƒì„±"""
        report_file = self.analysis_dir / "summary_report.txt"
        
        with open(report_file, 'w', encoding='utf-8') as f:
            f.write("="*70 + "\n")
            f.write("RFPilot ëª¨ë¸ ë¹„êµ ì‹¤í—˜ - ì¢…í•© ë¶„ì„ ë³´ê³ ì„œ\n")
            f.write("="*70 + "\n\n")
            
            # ë©”íƒ€ë°ì´í„°
            metadata = self.results['metadata']
            f.write(f"ì‹¤í—˜ ì¼ì‹œ: {metadata['timestamp']}\n")
            f.write(f"ë¶„í¬: {metadata['distribution']}\n")
            f.write(f"ë¹„êµ ëª¨ë¸: {', '.join(metadata['models'])}\n")
            f.write(f"ì´ ì§ˆë¬¸ ìˆ˜: {metadata['total_queries']}\n\n")
            
            # ì„±ê³µí•œ ê²°ê³¼ë§Œ
            df_success = self.df[self.df['success'] == True]
            
            # 1. ëª¨ë¸ë³„ í‰ê·  ì„±ëŠ¥
            f.write("\n" + "="*70 + "\n")
            f.write("1. ëª¨ë¸ë³„ í‰ê·  ì„±ëŠ¥\n")
            f.write("="*70 + "\n\n")
            
            for model in df_success['model'].unique():
                model_df = df_success[df_success['model'] == model]
                
                f.write(f"[{model}]\n")
                f.write(f"  - ì„±ê³µë¥ : {len(model_df)/len(self.df[self.df['model']==model])*100:.1f}%\n")
                f.write(f"  - í‰ê·  ì‘ë‹µ ì‹œê°„: {model_df['elapsed_time'].mean():.3f}ì´ˆ\n")
                f.write(f"  - í‰ê·  í† í°: {model_df['total_tokens'].mean():.1f}\n")
                f.write(f"  - RAG ì‚¬ìš©ë¥ : {model_df['used_retrieval'].sum()/len(model_df)*100:.1f}%\n\n")
            
            # 2. Distributionë³„ ì„±ëŠ¥
            f.write("\n" + "="*70 + "\n")
            f.write("2. Distributionë³„ ì„±ëŠ¥ ë¹„êµ\n")
            f.write("="*70 + "\n\n")
            
            for dist in df_success['distribution'].unique():
                dist_df = df_success[df_success['distribution'] == dist]
                
                f.write(f"[{dist}]\n")
                f.write(f"  - í‰ê·  ì‘ë‹µ ì‹œê°„: {dist_df['elapsed_time'].mean():.3f}ì´ˆ\n")
                f.write(f"  - í‰ê·  í† í°: {dist_df['total_tokens'].mean():.1f}\n\n")
            
            # 3. ê¶Œì¥ì‚¬í•­
            f.write("\n" + "="*70 + "\n")
            f.write("3. ë¶„ì„ ë° ê¶Œì¥ì‚¬í•­\n")
            f.write("="*70 + "\n\n")
            
            # ê°€ì¥ ë¹ ë¥¸ ëª¨ë¸
            fastest_model = df_success.groupby('model')['elapsed_time'].mean().idxmin()
            f.write(f"âš¡ ê°€ì¥ ë¹ ë¥¸ ëª¨ë¸: {fastest_model}\n")
            
            # ê°€ì¥ í† í°ì„ ì ê²Œ ì‚¬ìš©í•˜ëŠ” ëª¨ë¸
            efficient_model = df_success.groupby('model')['total_tokens'].mean().idxmin()
            f.write(f"ğŸ’¡ ê°€ì¥ íš¨ìœ¨ì ì¸ ëª¨ë¸ (í† í°): {efficient_model}\n")
            
            # RAGë¥¼ ê°€ì¥ ë§ì´ ì‚¬ìš©í•˜ëŠ” ëª¨ë¸
            rag_model = df_success.groupby('model')['used_retrieval'].sum().idxmax()
            f.write(f"ğŸ” RAGë¥¼ ê°€ì¥ ë§ì´ í™œìš©í•˜ëŠ” ëª¨ë¸: {rag_model}\n")
            
            f.write("\n" + "="*70 + "\n")
        
        print(f"âœ… ì¢…í•© ë³´ê³ ì„œ ì €ì¥: {report_file}")
    
    def run_all_analysis(self):
        """ëª¨ë“  ë¶„ì„ ì‹¤í–‰"""
        print("\n" + "="*70)
        print("ì „ì²´ ë¶„ì„ ì‹œì‘")
        print("="*70 + "\n")
        
        self.plot_time_comparison()
        self.plot_token_comparison()
        self.plot_rag_usage()
        self.plot_overfitting_analysis()
        self.generate_summary_report()
        
        print("\n" + "="*70)
        print("âœ… ëª¨ë“  ë¶„ì„ ì™„ë£Œ!")
        print(f"   ê²°ê³¼ ì €ì¥ ìœ„ì¹˜: {self.analysis_dir}")
        print("="*70 + "\n")


def main():
    """í…ŒìŠ¤íŠ¸ìš© ë©”ì¸ í•¨ìˆ˜"""
    import sys
    
    if len(sys.argv) < 2:
        print("ì‚¬ìš©ë²•: python analyze_results.py <result_json_path>")
        return
    
    result_file = sys.argv[1]
    
    analyzer = ResultAnalyzer(result_file)
    analyzer.run_all_analysis()


if __name__ == "__main__":
    main()