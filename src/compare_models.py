"""
3ê°€ì§€ ëª¨ë¸ ë¹„êµ ì‹¤í—˜

ë¹„êµ ëŒ€ìƒ:
1. QLoRA + RAG (ê¸°ì¡´ ì„œë¹„ìŠ¤)
2. QLoRA ë‹¨ë… (RAG ì œê±°)
3. Base + RAG (PEFT ì œê±°)

ì¸¡ì • ì§€í‘œ:
- ê³¼ì í•© ì—¬ë¶€ (In-Distribution vs Out-Distribution)
- ë‹µë³€ ì†ë„ (elapsed_time, retrieval_time, generation_time)
- í† í° ê°œìˆ˜ (total_tokens, prompt_tokens, completion_tokens)
"""

import os
import sys
import time
import json
import logging
from typing import Dict, List, Any
from datetime import datetime
from pathlib import Path

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ ê²½ë¡œ ì¶”ê°€
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

from src.utils.config import RAGConfig
from src.eval_dataset import EvalDataset

# ë¡œê¹… ì„¤ì •
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)


class ModelComparison:
    """ëª¨ë¸ ë¹„êµ ì‹¤í—˜ í´ë˜ìŠ¤"""
    
    def __init__(self, config=None, output_dir: str = "./results"):
        """ì´ˆê¸°í™”"""
        self.config = config or RAGConfig()
        self.output_dir = Path(output_dir)
        self.output_dir.mkdir(parents=True, exist_ok=True)
        
        # íƒ€ì„ìŠ¤íƒ¬í”„
        self.timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        
        # ë°ì´í„°ì…‹
        self.dataset = EvalDataset()
        
        # ëª¨ë¸ íŒŒì´í”„ë¼ì¸
        self.pipelines = {}
        
        logger.info(f"âœ… ModelComparison ì´ˆê¸°í™” ì™„ë£Œ")
        logger.info(f"   ê²°ê³¼ ì €ì¥ ê²½ë¡œ: {self.output_dir}")
    
    def load_models(self):
        """2ê°€ì§€ ëª¨ë¸ ë¡œë“œ (BaseëŠ” ì¶”í›„ GGUF ë³€í™˜ í›„ ì¶”ê°€ ì˜ˆì •)"""
        logger.info("\n" + "="*60)
        logger.info("ëª¨ë¸ ë¡œë”© ì‹œì‘ (2ê°œ ëª¨ë¸)")
        logger.info("="*60)
        
        try:
            # 1. QLoRA + RAG (ê¸°ì¡´)
            logger.info("\n[1/2] QLoRA + RAG ëª¨ë¸ ë¡œë”©...")
            from src.generator.generator_gguf import GGUFRAGPipeline
            self.pipelines['qlora_rag'] = GGUFRAGPipeline(config=self.config)
            logger.info("âœ… QLoRA + RAG ë¡œë“œ ì™„ë£Œ")
            
            # 2. QLoRA ë‹¨ë… (RAG ì œê±°)
            logger.info("\n[2/2] QLoRA ë‹¨ë… ëª¨ë¸ ë¡œë”©...")
            from src.generator.generator_gguf_no_rag import GGUFNoRAGPipeline
            self.pipelines['qlora_only'] = GGUFNoRAGPipeline(config=self.config)
            logger.info("âœ… QLoRA ë‹¨ë… ë¡œë“œ ì™„ë£Œ")
            
            # 3. Base + RAG (PEFT ì œê±°) - TODO: GGUF ë³€í™˜ í›„ ì¶”ê°€
            # logger.info("\n[3/3] Base + RAG ëª¨ë¸ ë¡œë”©...")
            # from src.generator.generator_gguf_base import GGUFBaseRAGPipeline
            # self.pipelines['base_rag'] = GGUFBaseRAGPipeline(config=self.config)
            # logger.info("âœ… Base + RAG ë¡œë“œ ì™„ë£Œ")
            logger.warning("\nâš ï¸ Base + RAG ìŠ¤í‚µ: Base ëª¨ë¸ GGUF ë³€í™˜ í›„ ì¶”ê°€ ì˜ˆì •")
            
        except Exception as e:
            logger.error(f"âŒ ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}")
            import traceback
            traceback.print_exc()
            raise
    
    def run_single_query(
        self, 
        model_name: str, 
        query: str, 
        query_info: Dict[str, Any]
    ) -> Dict[str, Any]:
        """ë‹¨ì¼ ì§ˆë¬¸ì— ëŒ€í•œ ëª¨ë¸ ì‹¤í–‰"""
        pipeline = self.pipelines[model_name]
        
        try:
            start_time = time.time()
            result = pipeline.generate_answer(query)
            total_time = time.time() - start_time
            
            # ê²°ê³¼ ì •ë¦¬
            return {
                'model': model_name,
                'query': query,
                'category': query_info.get('category', 'unknown'),
                'expected_type': query_info.get('expected_type', 'unknown'),
                'answer': result['answer'],
                'used_retrieval': result.get('used_retrieval', False),
                'query_type': result.get('query_type', 'unknown'),
                'search_mode': result.get('search_mode', 'none'),
                'elapsed_time': total_time,
                'model_elapsed_time': result.get('elapsed_time', 0),
                'usage': result.get('usage', {}),
                'sources_count': len(result.get('sources', [])),
                'success': True,
                'error': None
            }
        
        except Exception as e:
            logger.error(f"âŒ ì§ˆë¬¸ ì‹¤í–‰ ì‹¤íŒ¨ [{model_name}]: {e}")
            return {
                'model': model_name,
                'query': query,
                'category': query_info.get('category', 'unknown'),
                'expected_type': query_info.get('expected_type', 'unknown'),
                'answer': None,
                'used_retrieval': False,
                'query_type': 'error',
                'search_mode': 'none',
                'elapsed_time': 0,
                'model_elapsed_time': 0,
                'usage': {},
                'sources_count': 0,
                'success': False,
                'error': str(e)
            }
    
    def run_experiment(
        self, 
        distribution: str = 'all',
        save_results: bool = True
    ) -> Dict[str, List[Dict[str, Any]]]:
        """
        ì‹¤í—˜ ì‹¤í–‰
        
        Args:
            distribution: 'in', 'out', 'all'
            save_results: ê²°ê³¼ ì €ì¥ ì—¬ë¶€
        """
        logger.info("\n" + "="*60)
        logger.info("ì‹¤í—˜ ì‹œì‘")
        logger.info("="*60)
        
        # ë°ì´í„°ì…‹ ì¤€ë¹„
        if distribution == 'in':
            queries_dict = {'in_distribution': self.dataset.get_in_distribution()}
        elif distribution == 'out':
            queries_dict = {'out_distribution': self.dataset.get_out_distribution()}
        else:  # 'all'
            queries_dict = self.dataset.get_all_queries()
        
        # ê²°ê³¼ ì €ì¥
        all_results = {
            'metadata': {
                'timestamp': self.timestamp,
                'distribution': distribution,
                'models': list(self.pipelines.keys()),
                'total_queries': sum(len(v) for v in queries_dict.values())
            },
            'results': {}
        }
        
        # ê° ë¶„í¬ì— ëŒ€í•´ ì‹¤í—˜
        for dist_type, queries in queries_dict.items():
            logger.info(f"\n{'='*60}")
            logger.info(f"{dist_type.upper()} ì‹¤í—˜ ({len(queries)}ê°œ ì§ˆë¬¸)")
            logger.info(f"{'='*60}")
            
            dist_results = []
            
            # ê° ì§ˆë¬¸ì— ëŒ€í•´
            for i, query_info in enumerate(queries, 1):
                query = query_info['query']
                logger.info(f"\n[{i}/{len(queries)}] ì§ˆë¬¸: {query}")
                
                # ê° ëª¨ë¸ì— ëŒ€í•´
                for model_name in self.pipelines.keys():
                    logger.info(f"  â†’ {model_name} ì‹¤í–‰ ì¤‘...")
                    
                    result = self.run_single_query(model_name, query, query_info)
                    dist_results.append(result)
                    
                    if result['success']:
                        logger.info(f"     âœ… ì™„ë£Œ ({result['elapsed_time']:.2f}ì´ˆ)")
                    else:
                        logger.warning(f"     âŒ ì‹¤íŒ¨: {result['error']}")
            
            all_results['results'][dist_type] = dist_results
        
        # ê²°ê³¼ ì €ì¥
        if save_results:
            self._save_results(all_results)
        
        logger.info("\n" + "="*60)
        logger.info("âœ… ì‹¤í—˜ ì™„ë£Œ")
        logger.info("="*60 + "\n")
        
        return all_results
    
    def _save_results(self, results: Dict[str, Any]):
        """ê²°ê³¼ ì €ì¥"""
        # JSON íŒŒì¼ë¡œ ì €ì¥
        output_file = self.output_dir / f"results_{self.timestamp}.json"
        
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(results, f, ensure_ascii=False, indent=2)
        
        logger.info(f"ğŸ“ ê²°ê³¼ ì €ì¥: {output_file}")
        
        # ìš”ì•½ í†µê³„ ì €ì¥
        summary_file = self.output_dir / f"summary_{self.timestamp}.txt"
        self._save_summary(results, summary_file)
        
        logger.info(f"ğŸ“Š ìš”ì•½ ì €ì¥: {summary_file}")
    
    def _save_summary(self, results: Dict[str, Any], output_file: Path):
        """ìš”ì•½ í†µê³„ ì €ì¥"""
        with open(output_file, 'w', encoding='utf-8') as f:
            f.write("="*60 + "\n")
            f.write("ì‹¤í—˜ ê²°ê³¼ ìš”ì•½\n")
            f.write("="*60 + "\n\n")
            
            # ë©”íƒ€ë°ì´í„°
            metadata = results['metadata']
            f.write(f"íƒ€ì„ìŠ¤íƒ¬í”„: {metadata['timestamp']}\n")
            f.write(f"ë¶„í¬: {metadata['distribution']}\n")
            f.write(f"ëª¨ë¸: {', '.join(metadata['models'])}\n")
            f.write(f"ì´ ì§ˆë¬¸ ìˆ˜: {metadata['total_queries']}\n\n")
            
            # ê° ë¶„í¬ë³„ í†µê³„
            for dist_type, dist_results in results['results'].items():
                f.write(f"\n{'='*60}\n")
                f.write(f"{dist_type.upper()} ê²°ê³¼\n")
                f.write(f"{'='*60}\n\n")
                
                # ëª¨ë¸ë³„ë¡œ ê·¸ë£¹í™”
                model_stats = {}
                for result in dist_results:
                    model = result['model']
                    if model not in model_stats:
                        model_stats[model] = []
                    model_stats[model].append(result)
                
                # ê° ëª¨ë¸ë³„ í†µê³„
                for model, model_results in model_stats.items():
                    f.write(f"\n[{model}]\n")
                    
                    # ì„±ê³µ/ì‹¤íŒ¨
                    success_count = sum(1 for r in model_results if r['success'])
                    f.write(f"  ì„±ê³µ: {success_count}/{len(model_results)}\n")
                    
                    # í‰ê·  ì‹œê°„
                    avg_time = sum(r['elapsed_time'] for r in model_results if r['success']) / max(success_count, 1)
                    f.write(f"  í‰ê·  ì‹œê°„: {avg_time:.3f}ì´ˆ\n")
                    
                    # í‰ê·  í† í°
                    total_tokens = sum(r['usage'].get('total_tokens', 0) for r in model_results if r['success'])
                    avg_tokens = total_tokens / max(success_count, 1)
                    f.write(f"  í‰ê·  í† í°: {avg_tokens:.1f}\n")
                    
                    # RAG ì‚¬ìš©ë¥ 
                    rag_count = sum(1 for r in model_results if r['used_retrieval'])
                    f.write(f"  RAG ì‚¬ìš©: {rag_count}/{len(model_results)} ({rag_count/len(model_results)*100:.1f}%)\n")


def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    logger.info("="*60)
    logger.info("RFPilot ëª¨ë¸ ë¹„êµ ì‹¤í—˜")
    logger.info("="*60)
    
    # Config ë¡œë“œ
    config = RAGConfig()
    
    # ì‹¤í—˜ ì´ˆê¸°í™”
    experiment = ModelComparison(config=config, output_dir="./experiments/results")
    
    # ë°ì´í„°ì…‹ í™•ì¸
    experiment.dataset.print_summary()
    experiment.dataset.print_samples(n=3)
    
    # ëª¨ë¸ ë¡œë“œ
    experiment.load_models()
    
    # ì‹¤í—˜ ì‹¤í–‰
    # ì˜µì…˜ 1: ì „ì²´ ì‹¤í—˜
    results = experiment.run_experiment(distribution='all', save_results=True)
    
    # ì˜µì…˜ 2: In-Distributionë§Œ
    # results = experiment.run_experiment(distribution='in', save_results=True)
    
    # ì˜µì…˜ 3: Out-Distributionë§Œ
    # results = experiment.run_experiment(distribution='out', save_results=True)
    
    logger.info(f"\nâœ… ëª¨ë“  ì‹¤í—˜ ì™„ë£Œ!")
    logger.info(f"   ê²°ê³¼ ì €ì¥ ìœ„ì¹˜: {experiment.output_dir}")


if __name__ == "__main__":
    try:
        main()
    except KeyboardInterrupt:
        logger.info("\nâš ï¸ ì‚¬ìš©ìì— ì˜í•´ ì¤‘ë‹¨ë¨")
    except Exception as e:
        logger.error(f"\nâŒ ì‹¤í—˜ ì‹¤íŒ¨: {e}")
        import traceback
        traceback.print_exc()