from llama_cpp import Llama
from typing import Optional, Dict, Any, List
import logging
import time
import os

from src.utils.config import RAGConfig
from src.router.query_router import QueryRouter
from src.prompts.dynamic_prompts import PromptManager

# ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


class GGUFGenerator:
    """
    GGUF ê¸°ë°˜ Llama-3 ìƒì„±ê¸°
    
    llama.cppë¥¼ ì‚¬ìš©í•˜ì—¬ GGUF í¬ë§· ëª¨ë¸ì„ ë¡œë“œí•˜ê³ 
    ì…ì°° ê´€ë ¨ ì§ˆì˜ì‘ë‹µì„ ìˆ˜í–‰í•©ë‹ˆë‹¤.
    """
    
    def __init__(
        self,
        model_path: str,
        n_gpu_layers: int = 0,
        n_ctx: int = 8192,
        n_threads: int = 8,
        config = None,
        max_new_tokens: int = 256,
        temperature: float = 0.7,
        top_p: float = 0.9,
        system_prompt: str = "ë‹¹ì‹ ì€ RFP(ì œì•ˆìš”ì²­ì„œ) ë¶„ì„ ë° ìš”ì•½ ì „ë¬¸ê°€ì…ë‹ˆë‹¤."
    ):
        """ìƒì„±ê¸° ì´ˆê¸°í™”"""
        self.config = config or RAGConfig() 
        self.model_path = model_path
        self.n_gpu_layers = n_gpu_layers
        self.n_ctx = n_ctx
        self.n_threads = n_threads
        self.max_new_tokens = max_new_tokens
        self.temperature = temperature
        self.top_p = top_p
        self.system_prompt = system_prompt
        
        # ëª¨ë¸ (ë‚˜ì¤‘ì— ë¡œë“œ)
        self.model = None
        
        logger.info(f"GGUFGenerator ì´ˆê¸°í™” ì™„ë£Œ (Base ëª¨ë¸)")
    
    def load_model(self) -> None:
        """
        GGUF ëª¨ë¸ ë¡œë“œ
        
        âœ… Base ëª¨ë¸ ì‚¬ìš©: Configì—ì„œ BASE_MODEL_HUB_REPO ê°€ì ¸ì˜¤ê¸°
        """
        
        # ì¤‘ë³µ ë¡œë“œ ë°©ì§€
        if self.model is not None:
            logger.info("ëª¨ë¸ì´ ì´ë¯¸ ë¡œë“œë˜ì–´ ìˆìŠµë‹ˆë‹¤.")
            return
        
        try:
            # Configì—ì„œ USE_MODEL_HUB í™•ì¸
            use_model_hub = getattr(self.config, 'USE_MODEL_HUB', True)
            
            # Model Hub ì‚¬ìš© ì—¬ë¶€ì— ë”°ë¼ ê²½ë¡œ ê²°ì •
            if use_model_hub:
                # === Model Hubì—ì„œ ë‹¤ìš´ë¡œë“œ ===
                # âœ… Configì—ì„œ Base ëª¨ë¸ ì •ë³´ ê°€ì ¸ì˜¤ê¸°
                base_model_repo = getattr(
                    self.config, 
                    'BASE_MODEL_HUB_REPO', 
                    'Dongjin1203/Llama-3-Open-Ko-8B-GGUF'
                )
                base_model_filename = getattr(
                    self.config, 
                    'BASE_MODEL_HUB_FILENAME', 
                    'Llama-3-Open-Ko-8B-Q4_K_M.gguf'
                )
                model_cache_dir = getattr(self.config, 'MODEL_CACHE_DIR', '.cache/models')
                
                logger.info(f"ğŸ“¥ Base ëª¨ë¸ ë‹¤ìš´ë¡œë“œ: {base_model_repo}")
                logger.info(f"   íŒŒì¼ëª…: {base_model_filename}")
                
                from huggingface_hub import hf_hub_download
                
                model_path = hf_hub_download(
                    repo_id=base_model_repo,
                    filename=base_model_filename,
                    cache_dir=model_cache_dir,
                    local_dir=model_cache_dir,
                    local_dir_use_symlinks=False
                )
                
                logger.info(f"âœ… Base ëª¨ë¸ ë‹¤ìš´ë¡œë“œ ì™„ë£Œ: {model_path}")
                
            else:
                # === ë¡œì»¬ íŒŒì¼ ì‚¬ìš© ===
                model_path = self.model_path
                
                if not os.path.exists(model_path):
                    raise FileNotFoundError(
                        f"âŒ ë¡œì»¬ ëª¨ë¸ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {model_path}\n"
                        f"   USE_MODEL_HUB=trueë¡œ ì„¤ì •í•˜ê±°ë‚˜ ëª¨ë¸ íŒŒì¼ì„ ì¤€ë¹„í•˜ì„¸ìš”."
                    )
                
                logger.info(f"ğŸ“‚ ë¡œì»¬ Base ëª¨ë¸ ì‚¬ìš©: {model_path}")
            
            # === ê³µí†µ: ëª¨ë¸ ë¡œë“œ ===
            logger.info(f"ğŸš€ Base GGUF ëª¨ë¸ ë¡œë“œ ì¤‘...")
            logger.info(f"   GPU ë ˆì´ì–´: {self.n_gpu_layers}")
            logger.info(f"   ì»¨í…ìŠ¤íŠ¸: {self.n_ctx}")
            
            self.model = Llama(
                model_path=model_path,
                n_gpu_layers=self.n_gpu_layers,
                n_ctx=self.n_ctx,
                n_threads=self.n_threads,
                verbose=True,
            )
            
            # ì‹¤ì œ ì ìš©ëœ n_ctx í™•ì¸
            actual_n_ctx = self.model.n_ctx()
            logger.info("âœ… Base GGUF ëª¨ë¸ ë¡œë“œ ì™„ë£Œ!")
            logger.info(f"   - ëª¨ë¸: {base_model_repo if use_model_hub else 'local'}")
            logger.info(f"   - ì„¤ì •í•œ n_ctx: {self.n_ctx}")
            logger.info(f"   - ì‹¤ì œ n_ctx: {actual_n_ctx}")
            
            if actual_n_ctx < self.n_ctx:
                logger.warning(f"âš ï¸ n_ctxê°€ ì˜ˆìƒë³´ë‹¤ ì‘ìŠµë‹ˆë‹¤: {actual_n_ctx} < {self.n_ctx}")
            
        except FileNotFoundError as e:
            logger.error(f"âŒ ëª¨ë¸ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {e}")
            raise
        except Exception as e:
            logger.error(f"âŒ ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}")
            raise RuntimeError(f"ëª¨ë¸ ë¡œë“œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
    
    def format_prompt(
        self,
        question: str,
        context: Optional[str] = None,
        system_prompt: Optional[str] = None
    ) -> str:
        """GGUF ëª¨ë¸ìš© ê°„ë‹¨í•œ í”„ë¡¬í”„íŠ¸ í¬ë§·íŒ…"""
        if system_prompt is None:
            system_prompt = self.system_prompt
        
        if context is not None:
            user_message = f"ì°¸ê³  ë¬¸ì„œ:\n{context}\n\nì§ˆë¬¸: {question}"
        else:
            user_message = question
        
        formatted_prompt = f"""### ì‹œìŠ¤í…œ
{system_prompt}

### ì‚¬ìš©ì
{user_message}

### ë‹µë³€
"""
        
        return formatted_prompt
    
    def generate(
        self,
        prompt: str,
        max_new_tokens: Optional[int] = None,
        temperature: Optional[float] = None,
        top_p: Optional[float] = None,
    ) -> str:
        """í”„ë¡¬í”„íŠ¸ë¥¼ ì…ë ¥ë°›ì•„ ì‘ë‹µ ìƒì„±"""
        if self.model is None:
            raise RuntimeError(
                "ëª¨ë¸ì´ ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. load_model()ì„ ë¨¼ì € í˜¸ì¶œí•˜ì„¸ìš”."
            )
        
        if max_new_tokens is None:
            max_new_tokens = self.max_new_tokens
        if temperature is None:
            temperature = self.temperature
        if top_p is None:
            top_p = self.top_p
        
        try:
            logger.info(f"ğŸ”„ ìƒì„± ì‹œì‘ (max_tokens={max_new_tokens}, temp={temperature})")
            start_time = time.time()
            
            output = self.model(
                prompt,
                max_tokens=max_new_tokens,
                temperature=temperature,
                top_p=top_p,
                echo=False,
                stop=[
                    "###", "\n\n###", 
                    "### ì‚¬ìš©ì", "\nì‚¬ìš©ì:", 
                    "</s>",
                    "í•œêµ­ì–´ ë‹µë³€", "í•œêµ­ì–´ë¡œ ë‹µë³€", "ì§€ì¹¨:",
                    "ë¬¸ì¥", "(ë¬¸ì¥",
                    "\n\n",
                    "?",
                    "ìš”?", "ê¹Œ?", "ë‚˜ìš”?", "ìŠµë‹ˆê¹Œ?"
                ],
            )
            
            elapsed = time.time() - start_time
            logger.info(f"âœ… ìƒì„± ì™„ë£Œ: {elapsed:.2f}ì´ˆ")
            
            response = output['choices'][0]['text'].strip()
            
            logger.info(f"ğŸ“ ì‘ë‹µ ê¸¸ì´: {len(response)} ê¸€ì")
            return response
            
        except Exception as e:
            logger.error(f"âŒ ìƒì„± ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            raise RuntimeError(f"í…ìŠ¤íŠ¸ ìƒì„± ì‹¤íŒ¨: {e}")
    
    def chat(
        self,
        question: str,
        context: Optional[str] = None,
        system_prompt=None,
        **kwargs
    ) -> str:
        """ì§ˆë¬¸ì— ëŒ€í•œ ì‘ë‹µ ìƒì„±"""
        prompt = self.format_prompt(
            question=question,
            context=context,
            system_prompt=system_prompt
        )
        
        response = self.generate(prompt, **kwargs)
        
        return response


class GGUFBaseRAGPipeline:
    """
    Base ëª¨ë¸ + RAG íŒŒì´í”„ë¼ì¸
    
    âœ… Base ëª¨ë¸ ì‚¬ìš© (beomi/Llama-3-Open-Ko-8B)
    âœ… RAG ìœ ì§€
    âœ… ê¸°ì¡´ generator_gguf.pyì™€ ë™ì¼í•œ ê¸°ëŠ¥
    """
    
    def __init__(
        self,
        config=None,
        model: str = None,
        top_k: int = None,
        n_gpu_layers: int = None,
        n_ctx: int = None,
        n_threads: int = None,
        max_new_tokens: int = None,
        temperature: float = None,
        top_p: float = None,
        search_mode: str = None,
        alpha: float = None
    ):
        """ì´ˆê¸°í™”"""
        self.config = config or RAGConfig()
        
        # ê²€ìƒ‰ ì„¤ì •
        self.top_k = top_k or getattr(self.config, 'DEFAULT_TOP_K', 10)
        self.search_mode = search_mode or getattr(self.config, 'DEFAULT_SEARCH_MODE', 'hybrid_rerank')
        self.alpha = alpha if alpha is not None else getattr(self.config, 'DEFAULT_ALPHA', 0.5)
        
        # Retriever ì´ˆê¸°í™”
        logger.info("RAGRetriever ì´ˆê¸°í™” ì¤‘...")
        from src.retriever.retriever import RAGRetriever
        self.retriever = RAGRetriever(config=self.config)
        
        # GGUF ì„¤ì •
        gguf_n_gpu_layers = n_gpu_layers if n_gpu_layers is not None else getattr(self.config, 'GGUF_N_GPU_LAYERS', 35)
        gguf_n_ctx = n_ctx if n_ctx is not None else getattr(self.config, 'GGUF_N_CTX', 2048)
        gguf_n_threads = n_threads if n_threads is not None else getattr(self.config, 'GGUF_N_THREADS', 4)
        gguf_max_new_tokens = max_new_tokens if max_new_tokens is not None else getattr(self.config, 'GGUF_MAX_NEW_TOKENS', 512)
        gguf_temperature = temperature if temperature is not None else getattr(self.config, 'GGUF_TEMPERATURE', 0.7)
        gguf_top_p = top_p if top_p is not None else getattr(self.config, 'GGUF_TOP_P', 0.9)
        
        # ëª¨ë¸ ê²½ë¡œ (ì‚¬ìš© ì•ˆ í•¨, Hubì—ì„œ ë‹¤ìš´ë¡œë“œ)
        gguf_model_path = getattr(self.config, 'GGUF_MODEL_PATH', '.cache/models/llama-3-ko-8b.gguf')
        
        # ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸
        system_prompt = getattr(self.config, 'SYSTEM_PROMPT', 'ë‹¹ì‹ ì€ í•œêµ­ ê³µê³µê¸°ê´€ ì‚¬ì—…ì œì•ˆì„œ ë¶„ì„ ì „ë¬¸ê°€ì…ë‹ˆë‹¤.')
        
        # GGUFGenerator ì´ˆê¸°í™”
        logger.info("GGUFGenerator ì´ˆê¸°í™” ì¤‘... (Base ëª¨ë¸)")
        logger.info(f"   GPU ë ˆì´ì–´: {gguf_n_gpu_layers}")
        logger.info(f"   ì»¨í…ìŠ¤íŠ¸: {gguf_n_ctx}")
        
        self.generator = GGUFGenerator(
            model_path=gguf_model_path,
            n_gpu_layers=gguf_n_gpu_layers,
            n_ctx=gguf_n_ctx,
            n_threads=gguf_n_threads,
            config=self.config,
            max_new_tokens=gguf_max_new_tokens,
            temperature=gguf_temperature,
            top_p=gguf_top_p,
            system_prompt=system_prompt
        )
        
        # ëª¨ë¸ ë¡œë“œ
        logger.info("Base GGUF ëª¨ë¸ ë¡œë“œ ì¤‘...")
        self.generator.load_model()
        
        # Router
        self.router = QueryRouter()
        
        # ëŒ€í™” íˆìŠ¤í† ë¦¬
        self.chat_history: List[Dict] = []
        
        # ë§ˆì§€ë§‰ ê²€ìƒ‰ ê²°ê³¼
        self._last_retrieved_docs = []
        
        logger.info("âœ… GGUFBaseRAGPipeline ì´ˆê¸°í™” ì™„ë£Œ")
        logger.info(f"   - ê²€ìƒ‰ ëª¨ë“œ: {self.search_mode}")
        logger.info(f"   - ê¸°ë³¸ top_k: {self.top_k}")
    
    def _retrieve_and_format(self, query: str) -> str:
        """ê²€ìƒ‰ ìˆ˜í–‰ ë° ì»¨í…ìŠ¤íŠ¸ í¬ë§·íŒ…"""
        # ê²€ìƒ‰ ëª¨ë“œì— ë”°ë¼ ë¬¸ì„œ ê²€ìƒ‰
        if self.search_mode == "embedding":
            docs = self.retriever.search(query, top_k=self.top_k)
        elif self.search_mode == "embedding_rerank":
            docs = self.retriever.search_with_rerank(query, top_k=self.top_k)
        elif self.search_mode == "hybrid":
            docs = self.retriever.hybrid_search(
                query, top_k=self.top_k, alpha=self.alpha
            )
        elif self.search_mode == "hybrid_rerank":
            docs = self.retriever.hybrid_search_with_rerank(
                query, top_k=self.top_k, alpha=self.alpha
            )
        else:
            docs = self.retriever.search(query, top_k=self.top_k)
        
        # ë§ˆì§€ë§‰ ê²€ìƒ‰ ê²°ê³¼ ì €ì¥
        self._last_retrieved_docs = docs
        
        # ì»¨í…ìŠ¤íŠ¸ í¬ë§·íŒ…
        return self._format_context(docs)
    
    def _format_context(self, retrieved_docs: list) -> str:
        """ê²€ìƒ‰ëœ ë¬¸ì„œë¥¼ ì»¨í…ìŠ¤íŠ¸ë¡œ ë³€í™˜"""
        if not retrieved_docs:
            return "ê´€ë ¨ ë¬¸ì„œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
        
        context_parts = []
        max_context_chars = 8000
        
        current_length = 0
        for i, doc in enumerate(retrieved_docs, 1):
            doc_text = f"[ë¬¸ì„œ {i}]\n{doc['content']}\n"
            doc_length = len(doc_text)
            
            if current_length + doc_length > max_context_chars:
                logger.warning(f"âš ï¸ ì»¨í…ìŠ¤íŠ¸ ê¸¸ì´ ì œí•œ: {i-1}ê°œ ë¬¸ì„œë§Œ ì‚¬ìš©")
                break
            
            context_parts.append(doc_text)
            current_length += doc_length
        
        return "\n".join(context_parts)
    
    def _format_sources(self, retrieved_docs: list) -> list:
        """ê²€ìƒ‰ëœ ë¬¸ì„œë¥¼ sources í˜•ì‹ìœ¼ë¡œ ë³€í™˜"""
        sources = []
        for doc in retrieved_docs:
            source_info = {
                'content': doc['content'],
                'metadata': doc['metadata'],
                'filename': doc.get('filename', 'N/A'),
                'organization': doc.get('organization', 'N/A')
            }
            
            if 'rerank_score' in doc:
                source_info['score'] = doc['rerank_score']
                source_info['score_type'] = 'rerank'
            elif 'hybrid_score' in doc:
                source_info['score'] = doc['hybrid_score']
                source_info['score_type'] = 'hybrid'
            elif 'relevance_score' in doc:
                source_info['score'] = doc['relevance_score']
                source_info['score_type'] = 'embedding'
            else:
                source_info['score'] = 0
                source_info['score_type'] = 'unknown'
            
            sources.append(source_info)
        
        return sources
    
    def _estimate_usage(self, query: str, answer: str) -> dict:
        """í† í° ì‚¬ìš©ëŸ‰ ì¶”ì •"""
        prompt_tokens = len(query.split()) * 2
        completion_tokens = len(answer.split()) * 2
        
        return {
            'total_tokens': prompt_tokens + completion_tokens,
            'prompt_tokens': prompt_tokens,
            'completion_tokens': completion_tokens
        }
    
    def generate_answer(
        self,
        query: str,
        top_k: int = None,
        search_mode: str = None,
        alpha: float = None
    ) -> dict:
        """ë‹µë³€ ìƒì„± (Base ëª¨ë¸ + RAG)"""
        try:
            start_time = time.time()
            
            # íŒŒë¼ë¯¸í„° ì„¤ì •
            if top_k is not None:
                self.top_k = top_k
            if search_mode is not None:
                self.search_mode = search_mode
            if alpha is not None:
                self.alpha = alpha

            # Routerë¡œ ê²€ìƒ‰ ì—¬ë¶€ ê²°ì •
            classification = self.router.classify(query)
            query_type = classification['type']
            
            logger.info(f"ğŸ“ ë¶„ë¥˜: {query_type} (ì‹ ë¢°ë„: {classification['confidence']:.2f})")
            
            # íƒ€ì…ë³„ ì²˜ë¦¬
            if query_type in ['greeting', 'thanks', 'out_of_scope']:
                # ê²€ìƒ‰ ìŠ¤í‚µ
                context = None
                used_retrieval = False
                self._last_retrieved_docs = []
                
                # ë™ì  í”„ë¡¬í”„íŠ¸
                system_prompt = PromptManager.get_prompt(query_type, model_type="gguf")
                logger.info(f"â­ï¸ RAG ìŠ¤í‚µ: {query_type}")
            
            elif query_type == 'document':
                # RAG ìˆ˜í–‰
                context = self._retrieve_and_format(query)
                used_retrieval = True
                
                # ë™ì  í”„ë¡¬í”„íŠ¸
                system_prompt = PromptManager.get_prompt('document', model_type="gguf")
                logger.info(f"ğŸ” RAG ìˆ˜í–‰: {len(self._last_retrieved_docs)}ê°œ ë¬¸ì„œ")
            
            # ë‹µë³€ ìƒì„±
            answer = self.generator.chat(
                question=query,
                context=context,
                system_prompt=system_prompt
            )
            
            elapsed_time = time.time() - start_time
            
            # ëŒ€í™” íˆìŠ¤í† ë¦¬ ì¶”ê°€
            self.chat_history.append({"role": "user", "content": query})
            self.chat_history.append({"role": "assistant", "content": answer})
            
            # ê²°ê³¼ ë°˜í™˜
            return {
                'answer': answer,
                'sources': self._format_sources(self._last_retrieved_docs),
                'used_retrieval': used_retrieval,
                'query_type': query_type,
                'search_mode': self.search_mode if used_retrieval else 'direct',
                'routing_info': classification,
                'elapsed_time': elapsed_time,
                'usage': self._estimate_usage(query, answer)
            }
        
        except Exception as e:
            logger.error(f"âŒ ë‹µë³€ ìƒì„± ì‹¤íŒ¨: {e}")
            import traceback
            traceback.print_exc()
            raise RuntimeError(f"ë‹µë³€ ìƒì„± ì‹¤íŒ¨: {str(e)}") from e
    
    def chat(self, query: str) -> str:
        """ê°„ë‹¨í•œ ëŒ€í™” ì¸í„°í˜ì´ìŠ¤"""
        result = self.generate_answer(query)
        return result['answer']
    
    def clear_history(self):
        """ëŒ€í™” íˆìŠ¤í† ë¦¬ ì´ˆê¸°í™”"""
        self.chat_history = []
        logger.info("ğŸ—‘ï¸ ëŒ€í™” íˆìŠ¤í† ë¦¬ê°€ ì´ˆê¸°í™”ë˜ì—ˆìŠµë‹ˆë‹¤.")
    
    def get_history(self) -> List[Dict]:
        """ëŒ€í™” íˆìŠ¤í† ë¦¬ ë°˜í™˜"""
        return self.chat_history.copy()
    
    def set_search_config(
        self,
        search_mode: str = None,
        top_k: int = None,
        alpha: float = None
    ):
        """ê²€ìƒ‰ ì„¤ì • ë³€ê²½"""
        if search_mode is not None:
            self.search_mode = search_mode
        if top_k is not None:
            self.top_k = top_k
        if alpha is not None:
            self.alpha = alpha
        
        logger.info(
            f"ğŸ”§ ê²€ìƒ‰ ì„¤ì • ë³€ê²½: mode={self.search_mode}, "
            f"top_k={self.top_k}, alpha={self.alpha}"
        )