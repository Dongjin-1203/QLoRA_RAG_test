from llama_cpp import Llama
from typing import Optional, Dict, Any, List
import logging
import time
import os

from src.utils.config import RAGConfig
from src.router.query_router import QueryRouter
from src.prompts.dynamic_prompts import PromptManager

# ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


class GGUFGenerator:
    """
    GGUF ê¸°ë°˜ Llama-3 ìƒì„±ê¸°
    
    llama.cppë¥¼ ì‚¬ìš©í•˜ì—¬ GGUF í¬ë§· ëª¨ë¸ì„ ë¡œë“œí•˜ê³ 
    ì…ì°° ê´€ë ¨ ì§ˆì˜ì‘ë‹µì„ ìˆ˜í–‰í•©ë‹ˆë‹¤.
    """
    
    def __init__(
        self,
        model_path: str,
        n_gpu_layers: int = 0,
        n_ctx: int = 8192,
        n_threads: int = 8,
        config = None,
        max_new_tokens: int = 256,
        temperature: float = 0.7,
        top_p: float = 0.9,
        system_prompt: str = "ë‹¹ì‹ ì€ RFP(ì œì•ˆìš”ì²­ì„œ) ë¶„ì„ ë° ìš”ì•½ ì „ë¬¸ê°€ì…ë‹ˆë‹¤."
    ):
        """ìƒì„±ê¸° ì´ˆê¸°í™”"""
        self.config = config or RAGConfig() 
        self.model_path = model_path
        self.n_gpu_layers = n_gpu_layers
        self.n_ctx = n_ctx
        self.n_threads = n_threads
        self.max_new_tokens = max_new_tokens
        self.temperature = temperature
        self.top_p = top_p
        self.system_prompt = system_prompt
        
        # ëª¨ë¸ (ë‚˜ì¤‘ì— ë¡œë“œ)
        self.model = None
        
        logger.info(f"GGUFGenerator ì´ˆê¸°í™” ì™„ë£Œ")
    
    def load_model(self) -> None:
        """GGUF ëª¨ë¸ ë¡œë“œ"""
        
        # ì¤‘ë³µ ë¡œë“œ ë°©ì§€
        if self.model is not None:
            logger.info("ëª¨ë¸ì´ ì´ë¯¸ ë¡œë“œë˜ì–´ ìˆìŠµë‹ˆë‹¤.")
            return
        
        try:
            # Configì—ì„œ USE_MODEL_HUB í™•ì¸
            use_model_hub = getattr(self.config, 'USE_MODEL_HUB', True)
            
            # Model Hub ì‚¬ìš© ì—¬ë¶€ì— ë”°ë¼ ê²½ë¡œ ê²°ì •
            if use_model_hub:
                # === Model Hubì—ì„œ ë‹¤ìš´ë¡œë“œ ===
                model_hub_repo = getattr(self.config, 'MODEL_HUB_REPO', 'Dongjin1203/RFP_Documents_chatbot')
                model_hub_filename = getattr(self.config, 'MODEL_HUB_FILENAME', 'Llama-3-Open-Ko-8B.Q4_K_M.gguf')
                model_cache_dir = getattr(self.config, 'MODEL_CACHE_DIR', '.cache/models')
                
                logger.info(f"ğŸ“¥ Model Hubì—ì„œ ë‹¤ìš´ë¡œë“œ: {model_hub_repo}")
                
                from huggingface_hub import hf_hub_download
                
                model_path = hf_hub_download(
                    repo_id=model_hub_repo,
                    filename=model_hub_filename,
                    cache_dir=model_cache_dir,
                    local_dir=model_cache_dir,
                    local_dir_use_symlinks=False
                )
                
                logger.info(f"âœ… ë‹¤ìš´ë¡œë“œ ì™„ë£Œ: {model_path}")
                
            else:
                # === ë¡œì»¬ íŒŒì¼ ì‚¬ìš© ===
                model_path = self.model_path
                
                if not os.path.exists(model_path):
                    raise FileNotFoundError(
                        f"âŒ ë¡œì»¬ ëª¨ë¸ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {model_path}\n"
                        f"   USE_MODEL_HUB=trueë¡œ ì„¤ì •í•˜ê±°ë‚˜ ëª¨ë¸ íŒŒì¼ì„ ì¤€ë¹„í•˜ì„¸ìš”."
                    )
                
                logger.info(f"ğŸ“‚ ë¡œì»¬ ëª¨ë¸ ì‚¬ìš©: {model_path}")
            
            # === ê³µí†µ: ëª¨ë¸ ë¡œë“œ ===
            logger.info(f"ğŸš€ GGUF ëª¨ë¸ ë¡œë“œ ì¤‘...")
            logger.info(f"   GPU ë ˆì´ì–´: {self.n_gpu_layers}")
            logger.info(f"   ì»¨í…ìŠ¤íŠ¸: {self.n_ctx}")
            
            self.model = Llama(
                model_path=model_path,
                n_gpu_layers=self.n_gpu_layers,
                n_ctx=self.n_ctx,
                n_threads=self.n_threads,
                verbose=True,
            )
            
            # ì‹¤ì œ ì ìš©ëœ n_ctx í™•ì¸
            actual_n_ctx = self.model.n_ctx()
            logger.info("âœ… GGUF ëª¨ë¸ ë¡œë“œ ì™„ë£Œ!")
            logger.info(f"   - ì„¤ì •í•œ n_ctx: {self.n_ctx}")
            logger.info(f"   - ì‹¤ì œ n_ctx: {actual_n_ctx}")
            
            if actual_n_ctx < self.n_ctx:
                logger.warning(f"âš ï¸ n_ctxê°€ ì˜ˆìƒë³´ë‹¤ ì‘ìŠµë‹ˆë‹¤: {actual_n_ctx} < {self.n_ctx}")
                logger.warning(f"   ë©”ëª¨ë¦¬ ë¶€ì¡±ì¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤. n_gpu_layersë¥¼ ì¤„ì—¬ë³´ì„¸ìš”.")
            
        except FileNotFoundError as e:
            logger.error(f"âŒ ëª¨ë¸ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {e}")
            raise
        except Exception as e:
            logger.error(f"âŒ ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}")
            raise RuntimeError(f"ëª¨ë¸ ë¡œë“œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
    
    def format_prompt(
        self,
        question: str,
        context: Optional[str] = None,
        system_prompt: Optional[str] = None
    ) -> str:
        """GGUF ëª¨ë¸ìš© ê°„ë‹¨í•œ í”„ë¡¬í”„íŠ¸ í¬ë§·íŒ…"""
        # ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ ì„¤ì •
        if system_prompt is None:
            system_prompt = self.system_prompt
        
        # ì»¨í…ìŠ¤íŠ¸ í¬í•¨ ì—¬ë¶€
        if context is not None:
            user_message = f"ì°¸ê³  ë¬¸ì„œ:\n{context}\n\nì§ˆë¬¸: {question}"
        else:
            user_message = question
        
        # ê°„ë‹¨í•œ í•œêµ­ì–´ í…œí”Œë¦¿
        formatted_prompt = f"""### ì‹œìŠ¤í…œ
{system_prompt}

### ì‚¬ìš©ì
{user_message}

### ë‹µë³€
"""
        
        return formatted_prompt
    
    def generate(
        self,
        prompt: str,
        max_new_tokens: Optional[int] = None,
        temperature: Optional[float] = None,
        top_p: Optional[float] = None,
    ) -> str:
        """í”„ë¡¬í”„íŠ¸ë¥¼ ì…ë ¥ë°›ì•„ ì‘ë‹µ ìƒì„±"""
        # ëª¨ë¸ ë¡œë“œ í™•ì¸
        if self.model is None:
            raise RuntimeError(
                "ëª¨ë¸ì´ ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. load_model()ì„ ë¨¼ì € í˜¸ì¶œí•˜ì„¸ìš”."
            )
        
        # íŒŒë¼ë¯¸í„° ì„¤ì •
        if max_new_tokens is None:
            max_new_tokens = self.max_new_tokens
        if temperature is None:
            temperature = self.temperature
        if top_p is None:
            top_p = self.top_p
        
        try:
            logger.info(f"ğŸ”„ ìƒì„± ì‹œì‘ (max_tokens={max_new_tokens}, temp={temperature})")
            start_time = time.time()
            
            # ìƒì„±
            output = self.model(
                prompt,
                max_tokens=max_new_tokens,
                temperature=temperature,
                top_p=top_p,
                echo=False,
                stop=[
                    "###", "\n\n###", 
                    "### ì‚¬ìš©ì", "\nì‚¬ìš©ì:", 
                    "</s>",
                    "í•œêµ­ì–´ ë‹µë³€", "í•œêµ­ì–´ë¡œ ë‹µë³€", "ì§€ì¹¨:",
                    "ë¬¸ì¥", "(ë¬¸ì¥",
                    "\n\n",
                    "?",
                    "ìš”?", "ê¹Œ?", "ë‚˜ìš”?", "ìŠµë‹ˆê¹Œ?"
                ],
            )
            
            elapsed = time.time() - start_time
            logger.info(f"âœ… ìƒì„± ì™„ë£Œ: {elapsed:.2f}ì´ˆ")
            
            # ì‘ë‹µ ì¶”ì¶œ
            response = output['choices'][0]['text'].strip()
            
            logger.info(f"ğŸ“ ì‘ë‹µ ê¸¸ì´: {len(response)} ê¸€ì")
            return response
            
        except Exception as e:
            logger.error(f"âŒ ìƒì„± ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            raise RuntimeError(f"í…ìŠ¤íŠ¸ ìƒì„± ì‹¤íŒ¨: {e}")
    
    def chat(
        self,
        question: str,
        context: Optional[str] = None,
        system_prompt=None,
        **kwargs
    ) -> str:
        """ì§ˆë¬¸ì— ëŒ€í•œ ì‘ë‹µ ìƒì„± (í†µí•© ë©”ì„œë“œ)"""
        # í”„ë¡¬í”„íŠ¸ í¬ë§·íŒ…
        prompt = self.format_prompt(
            question=question,
            context=context,
            system_prompt=system_prompt
        )
        
        # ì‘ë‹µ ìƒì„±
        response = self.generate(prompt, **kwargs)
        
        return response


class GGUFNoRAGPipeline:
    """
    QLoRA ëª¨ë¸ ë‹¨ë… íŒŒì´í”„ë¼ì¸ (RAG ì œê±°)
    
    âœ… Retriever ì™„ì „ ì œê±°
    âœ… Routerë§Œ ìœ ì§€ (greeting/thanks ì²˜ë¦¬ìš©)
    âœ… ìˆœìˆ˜ ëª¨ë¸ ì„±ëŠ¥ë§Œ ì¸¡ì •
    """
    
    def __init__(
        self,
        config=None,
        model: str = None,
        top_k: int = None,
        n_gpu_layers: int = None,
        n_ctx: int = None,
        n_threads: int = None,
        max_new_tokens: int = None,
        temperature: float = None,
        top_p: float = None
    ):
        """ì´ˆê¸°í™”"""
        self.config = config or RAGConfig()
        
        # GGUF ì„¤ì •
        gguf_n_gpu_layers = n_gpu_layers if n_gpu_layers is not None else getattr(self.config, 'GGUF_N_GPU_LAYERS', 35)
        gguf_n_ctx = n_ctx if n_ctx is not None else getattr(self.config, 'GGUF_N_CTX', 2048)
        gguf_n_threads = n_threads if n_threads is not None else getattr(self.config, 'GGUF_N_THREADS', 4)
        gguf_max_new_tokens = max_new_tokens if max_new_tokens is not None else getattr(self.config, 'GGUF_MAX_NEW_TOKENS', 512)
        gguf_temperature = temperature if temperature is not None else getattr(self.config, 'GGUF_TEMPERATURE', 0.7)
        gguf_top_p = top_p if top_p is not None else getattr(self.config, 'GGUF_TOP_P', 0.9)
        
        # ëª¨ë¸ ê²½ë¡œ
        gguf_model_path = getattr(self.config, 'GGUF_MODEL_PATH', '.cache/models/llama-3-ko-8b.gguf')
        
        # ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸
        system_prompt = getattr(self.config, 'SYSTEM_PROMPT', 'ë‹¹ì‹ ì€ í•œêµ­ ê³µê³µê¸°ê´€ ì‚¬ì—…ì œì•ˆì„œ ë¶„ì„ ì „ë¬¸ê°€ì…ë‹ˆë‹¤.')
        
        # GGUFGenerator ì´ˆê¸°í™”
        logger.info("GGUFGenerator ì´ˆê¸°í™” ì¤‘... (RAG ì—†ìŒ)")
        logger.info(f"   GPU ë ˆì´ì–´: {gguf_n_gpu_layers}")
        logger.info(f"   ì»¨í…ìŠ¤íŠ¸: {gguf_n_ctx}")
        logger.info(f"   ìŠ¤ë ˆë“œ: {gguf_n_threads}")
        
        self.generator = GGUFGenerator(
            model_path=gguf_model_path,
            n_gpu_layers=gguf_n_gpu_layers,
            n_ctx=gguf_n_ctx,
            n_threads=gguf_n_threads,
            config=self.config,
            max_new_tokens=gguf_max_new_tokens,
            temperature=gguf_temperature,
            top_p=gguf_top_p,
            system_prompt=system_prompt
        )
        
        # ëª¨ë¸ ë¡œë“œ
        logger.info("GGUF ëª¨ë¸ ë¡œë“œ ì¤‘...")
        self.generator.load_model()
        
        # âœ… Retriever ì—†ìŒ (ì™„ì „ ì œê±°)
        self.retriever = None
        
        # Router (greeting/thanks ì²˜ë¦¬ìš©)
        self.router = QueryRouter()
        
        # ëŒ€í™” íˆìŠ¤í† ë¦¬
        self.chat_history: List[Dict] = []
        
        logger.info("âœ… GGUFNoRAGPipeline ì´ˆê¸°í™” ì™„ë£Œ (RAG ì œê±°)")
        logger.info("   - Retriever: âŒ ì—†ìŒ")
        logger.info("   - Router: âœ… ìˆìŒ (greeting/thanksìš©)")
    
    def _estimate_usage(self, query: str, answer: str) -> dict:
        """í† í° ì‚¬ìš©ëŸ‰ ì¶”ì •"""
        prompt_tokens = len(query.split()) * 2
        completion_tokens = len(answer.split()) * 2
        
        return {
            'total_tokens': prompt_tokens + completion_tokens,
            'prompt_tokens': prompt_tokens,
            'completion_tokens': completion_tokens
        }
    
    def generate_answer(
        self,
        query: str,
        top_k: int = None,
        search_mode: str = None,
        alpha: float = None
    ) -> dict:
        """
        ë‹µë³€ ìƒì„± (RAG ì—†ìŒ)
        
        Args:
            query: ì§ˆë¬¸
            top_k: ì‚¬ìš© ì•ˆ í•¨ (í˜¸í™˜ì„±ìš©)
            search_mode: ì‚¬ìš© ì•ˆ í•¨ (í˜¸í™˜ì„±ìš©)
            alpha: ì‚¬ìš© ì•ˆ í•¨ (í˜¸í™˜ì„±ìš©)
        
        Returns:
            dict: answer, sources, search_mode, usage, elapsed_time, used_retrieval
        """
        try:
            start_time = time.time()
            
            # Routerë¡œ ì§ˆë¬¸ ë¶„ë¥˜
            classification = self.router.classify(query)
            query_type = classification['type']
            
            logger.info(f"ğŸ“ ë¶„ë¥˜: {query_type} (ì‹ ë¢°ë„: {classification['confidence']:.2f})")
            
            # ë™ì  í”„ë¡¬í”„íŠ¸ ì„ íƒ
            if query_type in ['greeting', 'thanks', 'out_of_scope']:
                system_prompt = PromptManager.get_prompt(query_type, model_type="gguf")
            else:
                system_prompt = PromptManager.get_prompt('document', model_type="gguf")
            
            # âœ… í•­ìƒ RAG ì—†ì´ ìƒì„± (context=None)
            answer = self.generator.chat(
                question=query,
                context=None,  # âœ… ì»¨í…ìŠ¤íŠ¸ ì—†ìŒ
                system_prompt=system_prompt
            )
            
            elapsed_time = time.time() - start_time
            
            # ëŒ€í™” íˆìŠ¤í† ë¦¬ ì¶”ê°€
            self.chat_history.append({"role": "user", "content": query})
            self.chat_history.append({"role": "assistant", "content": answer})
            
            # ê²°ê³¼ ë°˜í™˜
            return {
                'answer': answer,
                'sources': [],  # âœ… ì†ŒìŠ¤ ì—†ìŒ
                'used_retrieval': False,  # âœ… ê²€ìƒ‰ ì•ˆ í•¨
                'query_type': query_type,
                'search_mode': 'none',  # âœ… ê²€ìƒ‰ ëª¨ë“œ ì—†ìŒ
                'routing_info': classification,
                'elapsed_time': elapsed_time,
                'usage': self._estimate_usage(query, answer)
            }
        
        except Exception as e:
            logger.error(f"âŒ ë‹µë³€ ìƒì„± ì‹¤íŒ¨: {e}")
            import traceback
            traceback.print_exc()
            raise RuntimeError(f"ë‹µë³€ ìƒì„± ì‹¤íŒ¨: {str(e)}") from e
    
    def chat(self, query: str) -> str:
        """ê°„ë‹¨í•œ ëŒ€í™” ì¸í„°í˜ì´ìŠ¤"""
        result = self.generate_answer(query)
        return result['answer']
    
    def clear_history(self):
        """ëŒ€í™” íˆìŠ¤í† ë¦¬ ì´ˆê¸°í™”"""
        self.chat_history = []
        logger.info("ğŸ—‘ï¸ ëŒ€í™” íˆìŠ¤í† ë¦¬ê°€ ì´ˆê¸°í™”ë˜ì—ˆìŠµë‹ˆë‹¤.")
    
    def get_history(self) -> List[Dict]:
        """ëŒ€í™” íˆìŠ¤í† ë¦¬ ë°˜í™˜"""
        return self.chat_history.copy()